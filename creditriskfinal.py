# -*- coding: utf-8 -*-
"""Creditriskfinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sCbx-HiabH1-9Mhi8XRKJjYdOq76e1HW

##Problem Statement:
The data is of 50000 customer id with various features.The task is to predict if the customer will default or not

##Aim:
At the end we intend to make a model which gives a certain level of accuracy with optimum number of features

##Methodology

1.   First we made the data suitable for analysis
2.   Then we performed some basic EDA to know about some features of the data
3.   We tried to make a new feature using isolation forest algorithm so that it becomes easier to learn pattern from data
4.   We used weight of evidence and information value for variable selection
5.   Then a baseline logistic model was used to predict the probability of default.
6.   Then random forest and light gbm model were used to improve the performance
7.   In this project we have not used train test split instead of that we have taken a new dataset for testing
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import RandomizedSearchCV
from sklearn.feature_selection import RFECV
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

#@title
data=pd.read_csv('/content/dcr_full.csv')

#@title
data.head()

#@title
l=list(data['id'])
l1=[]
for i in set(l):
  if l.count(i)>11:
    l1.append(i)

#@title
 df=data[data['id'].isin(list(set(l1)))]
 df.shape

#@title
def func(data1):
  data1.reset_index(inplace=True)
  d=data1[data1.index>len(data1['id'])-1-12]
  return d

#@title
d=pd.DataFrame()
d1=pd.DataFrame()
for i in set(l1):
    d=func(data[data['id']==i])
    d1=pd.concat([d,d1],axis=0)

#@title
d2=d1[d1['status_time']==2 ]

#@title
d1.drop(['index','res_time','lgd_time','recovery_res'],inplace=True,axis=1)

#@title
l=list(d2['id'])
finaldf=d1[~d1['id'].isin(l)]
final=pd.DataFrame()
final=finaldf

#@title
def func1(d):
  d.reset_index(inplace=True)
  d1=d[d.index>len(d['id'])-2]
  return d1

#@title
d5=pd.DataFrame()
d2=pd.DataFrame()
for i in set(list(final['id'])):
    d2=func1(final[final['id']==i])
    d5=pd.concat([d2,d3],axis=0)

d3=pd.read_csv('/content/creditcard.csv')

d3.head()

org=d3.drop(['Time','V10','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','V11','V12','V13','V14','V15','V16'],axis=1)

d4=org.iloc[0:100000,:]
test=org.iloc[100000:,:]

d4.columns=["FICO_time","interest_rate_time","uer_time","hpi_time","gdp_time","rate_time","LTV_time","LTV_orig_time","interest_rate_orig_time","balance_time",'class']
test.columns=["FICO_time","interest_rate_time","uer_time","hpi_time","gdp_time","rate_time","LTV_time","LTV_orig_time","interest_rate_orig_time","balance_time",'class']

"""##EDA"""

sns.boxplot(y='status_time',x="balance_time",data=d4,orient='h')

sns.boxplot(y='status_time',x="LTV_time",data=d4,orient='h')

sns.boxplot(y='status_time',x="interest_rate_time",data=d4,orient='h')

sns.boxplot(y='status_time',x="rate_time",data=d4,orient='h')

sns.boxplot(y='status_time',x="hpi_time",data=d4,orient='h')

sns.boxplot(y='status_time',x="gdp_time",data=d4,orient='h')

sns.boxplot(y='status_time',x="uer_time",data=d4,orient='h')

sns.boxplot(y='status_time',x="FICO_time",data=d4,orient='h')

"""###Observation:From the above boxplots we get an intuition that interest rate,hpi time,uer time can be significant variable because here the demarcations are stronger.This will help us in narrowing down are search while using information value."""

X=d4.iloc[:,d4.columns!='class']
y=d4.iloc[:,d4.columns=='class']
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,stratify=y)

X6=test.iloc[:,d4.columns!='class']
y6=test.iloc[:,d4.columns=='class']

"""###Vanilla Logistic regression model using all the original features"""

m1=LogisticRegression(class_weight='balanced')
m1.fit(X_train,y_train)

y_pred=m1.predict(X_test)
print(classification_report(y_test,y_pred))

y_pred1=m1.predict(X6)
print(classification_report(y6,y_pred1))

"""###The results are certainly very good but can we perform using lesser but important number of features to make the practical use of this model more easy

####Here we use isolation forest for anamoly detection and assign them a score which will help machine in finding the patterns more easily
"""

vanilladf=d4.copy()

from sklearn.ensemble import IsolationForest
model=IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.5),max_features=1.0)
model.fit(vanilladf[['uer_time']])

vanilladf['score']=model.decision_function(vanilladf[['uer_time']])
#X_test1['score']=model.decision_function(X_test1[['uer_time']])

vanilladf.head()

"""###Weight of evidence and Information value for finding the suitable features"""

!pip install scorecardpy

import scorecardpy as sc

bins1 = sc.woebin(d4, y="class")

l=X_train.columns

woedf=pd.DataFrame()
for i in l:
  dd=pd.DataFrame(bins[i])
  woedf=pd.concat([woedf,dd],axis=0)

woedf

data_woe = sc.woebin_ply(d4, bins1)
#data_woe.drop(['id_woe'],inplace=True,axis=1)

data_woe.shape

data_woe.head()

#@title
X1=data_woe.loc[:,data_woe.columns!='status_time']
Y1=data_woe.loc[:,data_woe.columns=='status_time']
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, random_state=42,stratify=Y1)

#@title
m2=LogisticRegression(max_iter=1000,class_weight='balanced')
m2.fit(X_train1,y_train1)

#@title
y_pred1=m2.predict(X_test1)
print(classification_report(y_test1,y_pred1))

#@title
X5=test_data_woe.iloc[:,2:18]
print(classification_report(y_final,m2.predict(X5)))

p={}
for i in l:
     p[i]=bins[i]['total_iv'][0]
p

useful1=[]
for i in p:
  if(p[i]>=1.7):
    useful1.append(i)

useful1

"""### 'LTV_time', 'interest_rate_time', 'hpi_time', 'gdp_time', 'uer_time' are the significant features"""

useful1.extend(['uer_time'])
useful1

comparedf=pd.DataFrame()
L=['LTV_time', 'interest_rate_time']
for i in useful1:
  if(i!='score') and i!='uer_time':
    comparedf[i+'_woe']=data_woe[i+'_woe']
comparedf['score']=model.decision_function(comparedf[['rate_time_woe']])
#comparedf['gdp_time']=vanilladf['gdp_time']
#comparedf['uer_time']=vanilladf['uer_time']
#comparedf['hpi_time']=vanilladf['hpi_time']
#comparedf['score']=vanilladf['score']
comparedf.dropna(axis=0,how='any')
comparedf['class']=d4['class']

"""###Out of sample data"""

#test_data=org.iloc[100000:200000,:]
#test_data.c=["LTV_time","interest_rate_time","rate_time","hpi_time","gdp_time","uer_time","FICO_time","LTV_orig_time","interest_rate_orig_time","balance_time",'status_time']
test_woe = sc.woebin_ply(test, bins1)
test['score']=model.decision_function(test[['uer_time']])

L=['interest_rate_time', 'hpi_time', 'FICO_time', 'rate_time']
finaltestdf=pd.DataFrame()
for i in L:
  finaltestdf[i]=test_woe[i+"_woe"]
finaltestdf['score']=test['score']
L1=['class']
for i in L1:
  finaltestdf[i]=test[i]
#finaltestdf.dropna(axis=0,how='any')
finaltestdf.head()

X_final=finaltestdf.loc[:,finaltestdf.columns!='class']
y_final=finaltestdf.loc[:,finaltestdf.columns=='class']

y_final.shape

X_final.head()

y_final.sum()

"""###Logistic regression model"""

X4=comparedf.loc[:,comparedf.columns != 'class']
y4=comparedf.loc[:,comparedf.columns=='class']
X_train2, X_test2, y_train2, y_test2 = train_test_split(X4, y4, random_state=42,stratify=y4)

m3=LogisticRegression(class_weight='balanced')
m3.fit(X_train2,y_train2)
y_pred2=m3.predict(X_test2)
print(classification_report(y_test2,y_pred2))

print(classification_report(y_final,m3.predict(X_final)))

"""###Light GBM"""

from lightgbm import LGBMClassifier
model1 = LGBMClassifier(max_depth=1,class_weight='balanced',n_estimators=1000,learning_rate=0.01)
model1.fit(X_train2, y_train2)

y8=model1.predict(X_test2)
print(classification_report(y_test2,y8))

"""###Here we see the performance has increased significantly hence we choose this model"""