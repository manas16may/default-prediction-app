# -*- coding: utf-8 -*-
"""credit risk new

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1biInIpLL9X3PmJj3CwpkpQzAJdyuKGcq

##Problem Statement:
The data is of 50000 customer id with various features.The task is to predict if the customer will default or not

##Aim:
At the end we intend to make a model which gives a certain level of accuracy with optimum number of features

##Methodology

1.   First we made the data suitable for analysis
2.   Then we performed some basic EDA to know about some features of the data
3.   We tried to make a new feature using isolation forest algorithm so that it becomes easier to learn pattern from data
4.   We used weight of evidence and information value for variable selection
5.   Then a baseline logistic model was used to predict the probability of default.
6.   Then random forest and light gbm model were used to improve the performance
7.   In this project we have not used train test split instead of that we have taken a new dataset for testing
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

#data=pd.read_csv('/content/dcr_full.csv')

#data.head()

#data.drop(['res_time','lgd_time','recovery_res'],inplace=True,axis=1)

#def func1(d):
  #d.reset_index(inplace=True)
  #d1=d[d.index>len(d['id'])-2]
  #return d1

#d3=pd.DataFrame()
#d2=pd.DataFrame()
#for i in set(list(data['id'])):
    #d2=func1(data[data['id']==i])
    #d3=pd.concat([d2,d3],axis=0)

#d3.drop(['index'],axis=1,inplace=True)

#d3['status_time'].value_counts()

#d3['status_time'].replace({2:0},inplace=True)
#d3['status_time'].value_counts()

#from google.colab import files
#d3.to_csv('filedataset.csv') 
#files.download('filedataset.csv')
d3=pd.read_csv('C:/Users/LENOVO/Downloads/filedataset.csv')
l=["balance_time","LTV_time","interest_rate_time","rate_time","hpi_time","gdp_time","uer_time","FICO_orig_time","LTV_orig_time","Interest_Rate_orig_time","hpi_orig_time"]

"""##EDA"""

"""sns.boxplot(y='status_time',x="balance_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="LTV_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="interest_rate_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="rate_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="hpi_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="gdp_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="uer_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="FICO_orig_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="LTV_orig_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="Interest_Rate_orig_time",data=d3,orient='h')

sns.boxplot(y='status_time',x="hpi_orig_time",data=d3,orient='h')"""

"""###Observation:From the above boxplots we get an intuition that interest rate,hpi time,uer time can be significant variable because here the demarcations are stronger.This will help us in narrowing down are search while using information value."""

mean1=d3['LTV_time'].mean()
d3['LTV_time'].fillna(value=mean1, inplace=True)
#d3.drop(['state_orig_time'],axis=1,inplace=True)

X=d3[[ 'balance_time', 'LTV_time', 'interest_rate_time', 'rate_time','hpi_time', 'gdp_time', 'uer_time', 'REtype_CO_orig_time','REtype_PU_orig_time', 'REtype_SF_orig_time', 'investor_orig_time', 'FICO_orig_time', 'LTV_orig_time','Interest_Rate_orig_time', 'hpi_orig_time']]
y=d3.iloc[:,-1]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,stratify=y)

y.shape

sum(y)

"""###Vanilla Logistic regression model using all the original features"""

m1=LogisticRegression(max_iter=1000,class_weight='balanced')
m1.fit(X_train,y_train)

y_pred=m1.predict(X_test)

lr1=m1.predict_proba(X_test)
lr1=lr1[:,1]

"""###The results are certainly very good but can we perform using lesser but important number of features to make the practical use of this model more easy"""

vanilladf=d3.copy()

"""####Here we use isolation forest for anamoly detection and assign them a score which will help machine in finding the patterns more easily """

from sklearn.ensemble import IsolationForest
model=IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.5),max_features=1.0)
model.fit(vanilladf)

vanilladf['score']=model.predict(vanilladf)
#X_test1['score']=model.predict(X_test1[['uer_time']])

vanilladf.head()



"""###Weight of evidence and Information value for finding the suitable features"""


import scorecardpy as sc

bins = sc.woebin(d3, y="status_time")

l=X_train.columns

woedf=pd.DataFrame()
for i in l:
  dd=pd.DataFrame(bins[i])
  woedf=pd.concat([woedf,dd],axis=0)

woedf

data_woe = sc.woebin_ply(d3, bins)
data_woe.drop(['id_woe'],inplace=True,axis=1)

data_woe.shape

data_woe.head()

X1=data_woe.loc[:,data_woe.columns!='status_time']
Y1=data_woe.loc[:,data_woe.columns=='status_time']
X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, Y1, random_state=42,stratify=Y1)

m2=LogisticRegression(max_iter=1000,class_weight='balanced')
m2.fit(X_train1,y_train1)

y_pred1=m2.predict(X_test1)

p={}
for i in l:
     p[i]=bins[i]['total_iv'][0]
p

useful=[]
for i in p:
  if(p[i]>=0.2):
    useful.append(i)

useful

"""### 'LTV_time', 'interest_rate_time', 'hpi_time', 'gdp_time', 'uer_time' are the significant features"""

useful.extend(['score'])
useful

vanilladf.reset_index(inplace=True)
comparedf=pd.DataFrame()
L=['LTV_time', 'interest_rate_time']
for i in useful:
  if(i!='score'):
    comparedf[i+'_woe']=data_woe[i+'_woe']
model=IsolationForest()
comparedf['score']=model.fit_predict(comparedf)
#comparedf['gdp_time']=vanilladf['gdp_time']
#comparedf['uer_time']=vanilladf['uer_time']
#comparedf['hpi_time']=vanilladf['hpi_time']
#comparedf['score']=vanilladf['score']
comparedf['status_time']=vanilladf['status_time']
comparedf.head()

"""###Out of sample data"""

"""from sklearn.utils import shuffle
test_data=pd.DataFrame(pd.read_csv('/content/test.csv'))
test_data = shuffle(test_data)
#test_data['score']=model.predict(test_data)
test_data_woe = sc.woebin_ply(test_data, bins)

test_data_woe=shuffle(test_data_woe)

L=useful
finaltestdf=pd.DataFrame()
for i in L:
  if i!='score':
    finaltestdf[i]=test_data_woe[i+'_woe']
#L1=['score','gdp_time','hpi_time','status_time','uer_time']
finaltestdf['score']=model.fit_predict(test_data)
L1=['status_time']
for i in L1:
  finaltestdf[i]=test_data[i]
finaltestdf

X_final=finaltestdf.loc[:,finaltestdf.columns!='status_time']
y_final=finaltestdf.loc[:,finaltestdf.columns=='status_time']""

y_final.shape

X_final.head()

y_final.sum()

"""###Logistic regression model"""

X4=comparedf.loc[:,comparedf.columns != 'status_time']
y4=comparedf.loc[:,'status_time']
X_train2, X_test2, y_train2, y_test2 = train_test_split(X4, y4, random_state=42,stratify=y4)

m3=LogisticRegression(max_iter=1000,class_weight='balanced')
m3.fit(X_train2,y_train2)
y_pred2=m3.predict(X_test2)


from sklearn.metrics import roc_auc_score
lr=m3.predict_proba(X_test2)
lr=lr[:,1]


"""###Light GBM"""

from lightgbm import LGBMClassifier
model1 = LGBMClassifier(max_depth=1,learning_rate=.1,class_weight='balanced',n_estimators=1000)
model1.fit(X_train2, y_train2)

y8=model1.predict(X_test2)


from sklearn.metrics import roc_auc_score
lr3=model1.predict_proba(X_test2)
lr3=lr3[:,1]
import pickle
pickle_out=open("creditrisk.pkl",'wb')
pickle.dump(model1,pickle_out)
pickle_out.close()

"""###Here we see the performance has increased significantly hence we choose this model"""